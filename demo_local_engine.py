"""
Demo: Aethel Local Engine
Demonstra√ß√£o da integra√ß√£o com Ollama para intelig√™ncia local.

Este script demonstra:
1. Detec√ß√£o autom√°tica do Ollama
2. Listagem de modelos instalados
3. Gera√ß√£o de c√≥digo com modelo local
4. Streaming de resposta para UX responsiva

Author: Kiro AI - Engenheiro-Chefe
Version: Epoch 4.0 "Neural Nexus"
Date: February 5, 2026
"""

from aethel.ai.local_engine import (
    LocalEngine,
    LocalInferenceRequest,
    OllamaNotAvailableError,
    ModelNotFoundError
)


def demo_basic_inference():
    """Demonstra infer√™ncia b√°sica com modelo local"""
    print("\n" + "=" * 80)
    print("DEMO 1: INFER√äNCIA B√ÅSICA")
    print("=" * 80)
    
    engine = LocalEngine()
    
    try:
        # Verificar Ollama
        engine.check_ollama_available()
        
        # Listar modelos
        models = engine.list_models()
        
        if not models:
            print("\n‚ùå Nenhum modelo instalado.")
            print("\nüì• Para instalar DeepSeek-Coder:")
            print("   ollama pull deepseek-coder:7b")
            return
        
        # Usar primeiro modelo dispon√≠vel
        model = models[0]
        print(f"\nüß† Usando modelo: {model.name}")
        
        # Criar requisi√ß√£o
        request = LocalInferenceRequest(
            prompt="Write a Python function to check if a number is prime",
            model=model.name,
            temperature=0.7,
            max_tokens=300
        )
        
        print(f"\nüí≠ Prompt: {request.prompt}")
        print("\n‚è≥ Gerando resposta...")
        
        # Gerar resposta
        response = engine.generate(request)
        
        print(f"\n‚úÖ Resposta gerada:")
        print("-" * 80)
        print(response.text)
        print("-" * 80)
        
        print(f"\nüìä M√©tricas:")
        print(f"   ‚Ä¢ Tokens gerados: {response.tokens_generated}")
        print(f"   ‚Ä¢ Lat√™ncia: {response.latency_ms:.0f}ms")
        print(f"   ‚Ä¢ Throughput: {response.tokens_per_second:.1f} tokens/s")
        
    except OllamaNotAvailableError as e:
        print(f"\n‚ùå Erro: {e}")
        print("\nüìñ Guia de instala√ß√£o:")
        print("   1. Visite: https://ollama.ai")
        print("   2. Baixe e instale Ollama")
        print("   3. Execute: ollama pull deepseek-coder:7b")
        print("   4. Execute este script novamente")


def demo_streaming():
    """Demonstra streaming de resposta para UX responsiva"""
    print("\n" + "=" * 80)
    print("DEMO 2: STREAMING (UX RESPONSIVA)")
    print("=" * 80)
    
    engine = LocalEngine()
    
    try:
        models = engine.list_models()
        
        if not models:
            print("\n‚ùå Nenhum modelo instalado.")
            return
        
        model = models[0]
        print(f"\nüß† Usando modelo: {model.name}")
        
        # Criar requisi√ß√£o
        request = LocalInferenceRequest(
            prompt="Explain how blockchain works in simple terms",
            model=model.name,
            temperature=0.8,
            max_tokens=200
        )
        
        print(f"\nüí≠ Prompt: {request.prompt}")
        print("\n‚ú® Resposta (streaming):")
        print("-" * 80)
        
        # Gerar com streaming
        for token in engine.stream_generate(request):
            print(token, end='', flush=True)
        
        print("\n" + "-" * 80)
        print("\n‚úÖ Streaming completo!")
        
    except OllamaNotAvailableError as e:
        print(f"\n‚ùå Erro: {e}")


def demo_model_management():
    """Demonstra gerenciamento de modelos"""
    print("\n" + "=" * 80)
    print("DEMO 3: GERENCIAMENTO DE MODELOS")
    print("=" * 80)
    
    engine = LocalEngine()
    
    try:
        # Listar modelos instalados
        models = engine.list_models()
        
        print(f"\nüìö Modelos instalados: {len(models)}")
        
        for model in models:
            print(f"\n   ü§ñ {model.name}")
            print(f"      ‚Ä¢ Tamanho: {model.size_gb:.2f} GB")
            print(f"      ‚Ä¢ Par√¢metros: ~{model.parameters/1e9:.1f}B")
            print(f"      ‚Ä¢ Fam√≠lia: {model.family}")
        
        # Mostrar modelos recomendados
        print("\nüí° Modelos recomendados:")
        recommendations = engine.get_recommended_models()
        
        for use_case, model_name in recommendations.items():
            installed = "‚úÖ" if any(m.name == model_name for m in models) else "‚ùå"
            print(f"   {installed} {use_case.capitalize()}: {model_name}")
        
        # Estat√≠sticas
        stats = engine.get_statistics()
        print(f"\nüìä Estat√≠sticas:")
        print(f"   ‚Ä¢ Ollama dispon√≠vel: {'‚úÖ' if stats['ollama_available'] else '‚ùå'}")
        print(f"   ‚Ä¢ Modelos instalados: {stats['models_installed']}")
        print(f"   ‚Ä¢ Espa√ßo total: {stats['total_size_gb']:.2f} GB")
        
    except OllamaNotAvailableError as e:
        print(f"\n‚ùå Erro: {e}")


def demo_code_generation():
    """Demonstra gera√ß√£o de c√≥digo Aethel"""
    print("\n" + "=" * 80)
    print("DEMO 4: GERA√á√ÉO DE C√ìDIGO AETHEL")
    print("=" * 80)
    
    engine = LocalEngine()
    
    try:
        models = engine.list_models()
        
        if not models:
            print("\n‚ùå Nenhum modelo instalado.")
            return
        
        # Preferir DeepSeek-Coder se dispon√≠vel
        model = None
        for m in models:
            if 'deepseek' in m.name.lower() or 'code' in m.name.lower():
                model = m
                break
        
        if not model:
            model = models[0]
        
        print(f"\nüß† Usando modelo: {model.name}")
        
        # Criar requisi√ß√£o com contexto Aethel
        request = LocalInferenceRequest(
            prompt="""Write an Aethel smart contract for a simple token transfer with conservation checking.
            
The contract should:
1. Define a transfer function
2. Check sender has sufficient balance
3. Ensure conservation (total supply unchanged)
4. Use Aethel syntax with 'ensure' statements""",
            model=model.name,
            temperature=0.5,  # Mais determin√≠stico para c√≥digo
            max_tokens=400,
            system="You are an expert in Aethel, a formally verified smart contract language. Generate clean, correct Aethel code."
        )
        
        print(f"\nüí≠ Gerando contrato Aethel...")
        print("\n‚è≥ Aguarde...")
        
        # Gerar c√≥digo
        response = engine.generate(request)
        
        print(f"\n‚úÖ C√≥digo gerado:")
        print("=" * 80)
        print(response.text)
        print("=" * 80)
        
        print(f"\nüìä M√©tricas:")
        print(f"   ‚Ä¢ Tokens: {response.tokens_generated}")
        print(f"   ‚Ä¢ Lat√™ncia: {response.latency_ms:.0f}ms")
        print(f"   ‚Ä¢ Throughput: {response.tokens_per_second:.1f} tokens/s")
        
        print(f"\nüí° Pr√≥ximo passo: Enviar para Judge para verifica√ß√£o formal!")
        
    except OllamaNotAvailableError as e:
        print(f"\n‚ùå Erro: {e}")


def main():
    """Executa todas as demos"""
    print("\n" + "=" * 80)
    print("üåå AETHEL LOCAL ENGINE - DEMONSTRA√á√ÉO COMPLETA")
    print("=" * 80)
    print("\nEste demo mostra a integra√ß√£o da Aethel com Ollama para")
    print("intelig√™ncia local sem depend√™ncia de APIs externas.")
    print("\n" + "=" * 80)
    
    # Demo 1: Infer√™ncia b√°sica
    demo_basic_inference()
    
    # Demo 2: Streaming
    input("\n\n‚è∏Ô∏è  Pressione ENTER para continuar para Demo 2...")
    demo_streaming()
    
    # Demo 3: Gerenciamento de modelos
    input("\n\n‚è∏Ô∏è  Pressione ENTER para continuar para Demo 3...")
    demo_model_management()
    
    # Demo 4: Gera√ß√£o de c√≥digo Aethel
    input("\n\n‚è∏Ô∏è  Pressione ENTER para continuar para Demo 4...")
    demo_code_generation()
    
    print("\n" + "=" * 80)
    print("‚úÖ DEMONSTRA√á√ÉO COMPLETA!")
    print("=" * 80)
    print("\nüöÄ Pr√≥ximos passos:")
    print("   1. Implementar Teacher APIs (GPT-4, Claude, DeepSeek)")
    print("   2. Implementar Destilador Aut√¥nomo (compara√ß√£o + verifica√ß√£o)")
    print("   3. Implementar LoRA Training (fine-tuning local)")
    print("   4. Implementar Inference Sharding (distribui√ß√£o P2P)")
    print("\nüåå O Neural Nexus est√° nascendo!")
    print("=" * 80)


if __name__ == "__main__":
    main()
