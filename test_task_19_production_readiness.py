"""
Task 19: Final Production Readiness Checkpoint
Validates all completion criteria for Diotec360-Pilot v3.7

This script verifies:
1. All tests passing (unit, property, integration)
2. Performance targets met (95% < 250ms)
3. Error handling validated
4. Documentation complete
5. Ready for production deployment
"""

import subprocess
import sys
import os
import json
from pathlib import Path
from typing import List, Tuple, Dict
import time


class ProductionReadinessValidator:
    """Validates production readiness for Diotec360-Pilot v3.7"""
    
    def __init__(self):
        self.results = {
            "tests_passing": False,
            "performance_targets_met": False,
            "error_handling_validated": False,
            "documentation_complete": False,
            "production_ready": False,
            "details": {}
        }
    
    def validate_all_tests(self) -> Tuple[bool, Dict]:
        """Validate that all tests pass"""
        print("\n" + "="*80)
        print("VALIDATING: All Tests Passing")
        print("="*80)
        
        test_files = [
            # API Tests
            "test_autopilot_api.py",
            
            # Autopilot Engine Tests
            "test_task_6_autopilot_engine.py",
            
            # Traffic Light Tests
            "test_task_7_traffic_light.py",
            
            # Checkpoint Tests
            "test_task_4_checkpoint.py",
            "test_task_8_checkpoint.py",
            
            # Corrections Tests
            "test_task_9_corrections.py",
            
            # Performance Tests
            "test_task_11_performance.py",
            
            # Error Handling Tests
            "test_task_12_error_handling.py",
            
            # UI Polish Tests
            "test_task_13_ui_polish.py",
            
            # UI Consistency Tests
            "test_task_14_ui_consistency.py",
            
            # Integration Tests
            "test_task_17_integration.py",
        ]
        
        results = {
            "total": len(test_files),
            "passed": 0,
            "failed": 0,
            "skipped": 0,
            "details": []
        }
        
        for test_file in test_files:
            if not os.path.exists(test_file):
                print(f"‚ö†Ô∏è  Test file not found: {test_file}")
                results["skipped"] += 1
                results["details"].append({
                    "file": test_file,
                    "status": "skipped",
                    "reason": "File not found"
                })
                continue
            
            print(f"\nüìã Running: {test_file}")
            try:
                result = subprocess.run(
                    [sys.executable, "-m", "pytest", test_file, "-v", "--tb=short"],
                    capture_output=True,
                    text=True,
                    timeout=120
                )
                
                if result.returncode == 0:
                    print(f"‚úÖ PASSED: {test_file}")
                    results["passed"] += 1
                    results["details"].append({
                        "file": test_file,
                        "status": "passed"
                    })
                else:
                    print(f"‚ùå FAILED: {test_file}")
                    print(f"Output: {result.stdout[-500:]}")
                    results["failed"] += 1
                    results["details"].append({
                        "file": test_file,
                        "status": "failed",
                        "output": result.stdout[-500:]
                    })
            except subprocess.TimeoutExpired:
                print(f"‚è±Ô∏è  TIMEOUT: {test_file}")
                results["failed"] += 1
                results["details"].append({
                    "file": test_file,
                    "status": "timeout"
                })
            except Exception as e:
                print(f"‚ùå ERROR: {test_file} - {str(e)}")
                results["failed"] += 1
                results["details"].append({
                    "file": test_file,
                    "status": "error",
                    "error": str(e)
                })
        
        all_passed = results["failed"] == 0 and results["passed"] > 0
        
        print(f"\n{'='*80}")
        print(f"TEST SUMMARY:")
        print(f"  Total: {results['total']}")
        print(f"  Passed: {results['passed']} ‚úÖ")
        print(f"  Failed: {results['failed']} ‚ùå")
        print(f"  Skipped: {results['skipped']} ‚ö†Ô∏è")
        print(f"{'='*80}")
        
        return all_passed, results
    
    def validate_performance_targets(self) -> Tuple[bool, Dict]:
        """Validate performance targets are met"""
        print("\n" + "="*80)
        print("VALIDATING: Performance Targets (95% < 250ms)")
        print("="*80)
        
        # Check if performance test results exist
        perf_test_file = "test_task_11_performance.py"
        
        if not os.path.exists(perf_test_file):
            print(f"‚ö†Ô∏è  Performance test file not found: {perf_test_file}")
            return False, {"status": "not_found"}
        
        print(f"\nüìä Running performance validation...")
        try:
            result = subprocess.run(
                [sys.executable, "-m", "pytest", perf_test_file, "-v", "-k", "response_time"],
                capture_output=True,
                text=True,
                timeout=180
            )
            
            passed = result.returncode == 0
            
            if passed:
                print("‚úÖ Performance targets MET: 95% of requests < 250ms")
            else:
                print("‚ùå Performance targets NOT MET")
                print(f"Output: {result.stdout[-500:]}")
            
            return passed, {
                "status": "passed" if passed else "failed",
                "output": result.stdout[-500:]
            }
        except Exception as e:
            print(f"‚ùå Error validating performance: {str(e)}")
            return False, {"status": "error", "error": str(e)}
    
    def validate_error_handling(self) -> Tuple[bool, Dict]:
        """Validate error handling is properly implemented"""
        print("\n" + "="*80)
        print("VALIDATING: Error Handling")
        print("="*80)
        
        error_test_file = "test_task_12_error_handling.py"
        
        if not os.path.exists(error_test_file):
            print(f"‚ö†Ô∏è  Error handling test file not found: {error_test_file}")
            return False, {"status": "not_found"}
        
        print(f"\nüõ°Ô∏è  Running error handling validation...")
        try:
            result = subprocess.run(
                [sys.executable, "-m", "pytest", error_test_file, "-v"],
                capture_output=True,
                text=True,
                timeout=120
            )
            
            passed = result.returncode == 0
            
            if passed:
                print("‚úÖ Error handling VALIDATED")
            else:
                print("‚ùå Error handling validation FAILED")
                print(f"Output: {result.stdout[-500:]}")
            
            return passed, {
                "status": "passed" if passed else "failed",
                "output": result.stdout[-500:]
            }
        except Exception as e:
            print(f"‚ùå Error validating error handling: {str(e)}")
            return False, {"status": "error", "error": str(e)}
    
    def validate_documentation(self) -> Tuple[bool, Dict]:
        """Validate documentation is complete"""
        print("\n" + "="*80)
        print("VALIDATING: Documentation Complete")
        print("="*80)
        
        required_docs = [
            "docs/api/autopilot-api.md",
            "docs/frontend/monaco-editor-integration.md",
            "docs/deployment/Diotec360-pilot-deployment.md",
            ".kiro/specs/Diotec360-pilot-v3-7/requirements.md",
            ".kiro/specs/Diotec360-pilot-v3-7/design.md",
            ".kiro/specs/Diotec360-pilot-v3-7/tasks.md"
        ]
        
        results = {
            "total": len(required_docs),
            "found": 0,
            "missing": [],
            "details": []
        }
        
        for doc in required_docs:
            if os.path.exists(doc):
                print(f"‚úÖ Found: {doc}")
                results["found"] += 1
                results["details"].append({
                    "file": doc,
                    "status": "found"
                })
            else:
                print(f"‚ùå Missing: {doc}")
                results["missing"].append(doc)
                results["details"].append({
                    "file": doc,
                    "status": "missing"
                })
        
        all_found = len(results["missing"]) == 0
        
        print(f"\n{'='*80}")
        print(f"DOCUMENTATION SUMMARY:")
        print(f"  Total Required: {results['total']}")
        print(f"  Found: {results['found']} ‚úÖ")
        print(f"  Missing: {len(results['missing'])} ‚ùå")
        print(f"{'='*80}")
        
        return all_found, results
    
    def validate_implementation_files(self) -> Tuple[bool, Dict]:
        """Validate all implementation files exist"""
        print("\n" + "="*80)
        print("VALIDATING: Implementation Files")
        print("="*80)
        
        required_files = [
            # Backend
            "api/autopilot.py",
            "Diotec360/ai/autopilot_engine.py",
            
            # Frontend
            "frontend/components/MonacoAutopilot.tsx",
            "frontend/lib/autopilotClient.ts",
        ]
        
        results = {
            "total": len(required_files),
            "found": 0,
            "missing": []
        }
        
        for file in required_files:
            if os.path.exists(file):
                print(f"‚úÖ Found: {file}")
                results["found"] += 1
            else:
                print(f"‚ùå Missing: {file}")
                results["missing"].append(file)
        
        all_found = len(results["missing"]) == 0
        
        print(f"\n{'='*80}")
        print(f"IMPLEMENTATION FILES SUMMARY:")
        print(f"  Total Required: {results['total']}")
        print(f"  Found: {results['found']} ‚úÖ")
        print(f"  Missing: {len(results['missing'])} ‚ùå")
        print(f"{'='*80}")
        
        return all_found, results
    
    def run_validation(self) -> Dict:
        """Run complete production readiness validation"""
        print("\n" + "="*80)
        print("Diotec360-PILOT V3.7 - PRODUCTION READINESS VALIDATION")
        print("="*80)
        print(f"Started: {time.strftime('%Y-%m-%d %H:%M:%S')}")
        
        # 1. Validate implementation files
        impl_passed, impl_results = self.validate_implementation_files()
        self.results["details"]["implementation_files"] = impl_results
        
        # 2. Validate all tests
        tests_passed, test_results = self.validate_all_tests()
        self.results["tests_passing"] = tests_passed
        self.results["details"]["tests"] = test_results
        
        # 3. Validate performance targets
        perf_passed, perf_results = self.validate_performance_targets()
        self.results["performance_targets_met"] = perf_passed
        self.results["details"]["performance"] = perf_results
        
        # 4. Validate error handling
        error_passed, error_results = self.validate_error_handling()
        self.results["error_handling_validated"] = error_passed
        self.results["details"]["error_handling"] = error_results
        
        # 5. Validate documentation
        docs_passed, docs_results = self.validate_documentation()
        self.results["documentation_complete"] = docs_passed
        self.results["details"]["documentation"] = docs_results
        
        # Determine overall production readiness
        self.results["production_ready"] = (
            impl_passed and
            tests_passed and
            perf_passed and
            error_passed and
            docs_passed
        )
        
        return self.results
    
    def print_final_report(self):
        """Print final production readiness report"""
        print("\n" + "="*80)
        print("FINAL PRODUCTION READINESS REPORT")
        print("="*80)
        print(f"Completed: {time.strftime('%Y-%m-%d %H:%M:%S')}")
        print()
        
        criteria = [
            ("Implementation Files", self.results["details"].get("implementation_files", {}).get("found", 0) == self.results["details"].get("implementation_files", {}).get("total", 0)),
            ("All Tests Passing", self.results["tests_passing"]),
            ("Performance Targets Met", self.results["performance_targets_met"]),
            ("Error Handling Validated", self.results["error_handling_validated"]),
            ("Documentation Complete", self.results["documentation_complete"])
        ]
        
        for criterion, passed in criteria:
            status = "‚úÖ PASS" if passed else "‚ùå FAIL"
            print(f"  {criterion:.<50} {status}")
        
        print()
        print("="*80)
        
        if self.results["production_ready"]:
            print("üéâ PRODUCTION READY: Diotec360-Pilot v3.7 is ready for deployment!")
            print("="*80)
            print("\nNext Steps:")
            print("  1. Review deployment guide: docs/deployment/Diotec360-pilot-deployment.md")
            print("  2. Configure production environment variables")
            print("  3. Deploy backend API to production")
            print("  4. Deploy frontend to production")
            print("  5. Monitor performance and error rates")
            print("  6. Collect user feedback")
        else:
            print("‚ö†Ô∏è  NOT PRODUCTION READY: Some criteria not met")
            print("="*80)
            print("\nRequired Actions:")
            
            if not self.results["details"].get("implementation_files", {}).get("found", 0) == self.results["details"].get("implementation_files", {}).get("total", 0):
                print("  - Complete missing implementation files")
            
            if not self.results["tests_passing"]:
                print("  - Fix failing tests")
                failed_tests = [d for d in self.results["details"].get("tests", {}).get("details", []) if d.get("status") == "failed"]
                for test in failed_tests[:5]:  # Show first 5
                    print(f"    ‚Ä¢ {test['file']}")
            
            if not self.results["performance_targets_met"]:
                print("  - Optimize performance to meet 250ms target")
            
            if not self.results["error_handling_validated"]:
                print("  - Fix error handling issues")
            
            if not self.results["documentation_complete"]:
                print("  - Complete missing documentation")
                missing_docs = self.results["details"].get("documentation", {}).get("missing", [])
                for doc in missing_docs[:5]:  # Show first 5
                    print(f"    ‚Ä¢ {doc}")
        
        print()
        
        # Save results to file
        with open("TASK_19_PRODUCTION_READINESS_REPORT.json", "w") as f:
            json.dump(self.results, f, indent=2)
        
        print("üìÑ Detailed report saved to: TASK_19_PRODUCTION_READINESS_REPORT.json")
        print()


def main():
    """Main validation entry point"""
    validator = ProductionReadinessValidator()
    results = validator.run_validation()
    validator.print_final_report()
    
    # Exit with appropriate code
    sys.exit(0 if results["production_ready"] else 1)


if __name__ == "__main__":
    main()
