"""
Copyright 2024 Dion√≠sio Sebasti√£o Barros / DIOTEC 360

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
"""

"""
Aethel Local Engine - Interface com Ollama
O c√©rebro local que pensa sem internet.

Este m√≥dulo implementa a interface com Ollama para execu√ß√£o local de modelos de IA.
Permite que a Aethel execute DeepSeek-Coder, Llama, e outros modelos localmente,
sem depend√™ncia de APIs externas.

Research Foundation:
- Ollama: Local LLM runtime (https://ollama.ai)
- DeepSeek-Coder: Efficient coding model (https://github.com/deepseek-ai/DeepSeek-Coder)
- LoRA: Low-Rank Adaptation for efficient fine-tuning

Author: Kiro AI - Engenheiro-Chefe
Version: Epoch 4.0 "Neural Nexus"
Date: February 5, 2026
"""

from dataclasses import dataclass, field, asdict
from typing import List, Optional, Iterator, Dict, Any
import time
import requests
import json
from pathlib import Path


@dataclass
class OllamaModel:
    """
    Representa um modelo de IA dispon√≠vel no Ollama.
    
    Attributes:
        name: Nome do modelo (ex: "deepseek-coder:7b", "llama3:8b")
        size_gb: Tamanho do modelo em gigabytes
        parameters: N√∫mero de par√¢metros (ex: 7B, 70B)
        context_length: Tamanho m√°ximo do contexto em tokens
        installed: Se o modelo est√° instalado localmente
        family: Fam√≠lia do modelo (deepseek, llama, mistral, etc.)
        modified_at: Timestamp da √∫ltima modifica√ß√£o
    """
    name: str
    size_gb: float = 0.0
    parameters: int = 0
    context_length: int = 4096
    installed: bool = False
    family: str = "unknown"
    modified_at: Optional[float] = None
    
    def to_dict(self) -> Dict[str, Any]:
        """Converte para dicion√°rio"""
        return asdict(self)


@dataclass
class LocalInferenceRequest:
    """
    Requisi√ß√£o de infer√™ncia local.
    
    Attributes:
        prompt: Texto de entrada para o modelo
        model: Nome do modelo a usar
        temperature: Controla aleatoriedade (0.0 = determin√≠stico, 1.0 = criativo)
        max_tokens: N√∫mero m√°ximo de tokens a gerar
        stream: Se deve retornar resposta em streaming
        system: Prompt de sistema (opcional)
        context: Contexto anterior para continua√ß√£o (opcional)
    """
    prompt: str
    model: str
    temperature: float = 0.7
    max_tokens: int = 2048
    stream: bool = False
    system: Optional[str] = None
    context: Optional[List[int]] = None


@dataclass
class LocalInferenceResponse:
    """
    Resposta de infer√™ncia local.
    
    Attributes:
        text: Texto gerado pelo modelo
        model: Nome do modelo usado
        tokens_generated: N√∫mero de tokens gerados
        latency_ms: Lat√™ncia total em milissegundos
        tokens_per_second: Throughput (tokens/segundo)
        context: Contexto para continua√ß√£o (opcional)
        done: Se a gera√ß√£o est√° completa
    """
    text: str
    model: str
    tokens_generated: int
    latency_ms: float
    tokens_per_second: float
    context: Optional[List[int]] = None
    done: bool = True
    
    def to_dict(self) -> Dict[str, Any]:
        """Converte para dicion√°rio"""
        return asdict(self)


class OllamaNotAvailableError(Exception):
    """Exce√ß√£o quando Ollama n√£o est√° dispon√≠vel"""
    pass


class ModelNotFoundError(Exception):
    """Exce√ß√£o quando modelo n√£o est√° instalado"""
    pass


class LocalEngine:
    """
    Motor de Intelig√™ncia Local - Interface com Ollama.
    
    Este √© o c√©rebro local da Aethel. Ele permite executar modelos de IA
    localmente sem depend√™ncia de APIs externas. Suporta:
    
    - DeepSeek-Coder (7B, 33B)
    - Llama 3 (8B, 70B)
    - Mistral (7B)
    - CodeLlama (7B, 13B, 34B)
    
    O Local Engine √© a base para:
    1. Destila√ß√£o Aut√¥noma (aprender com GPT-4/Claude)
    2. Inference Sharding (distribuir modelo pela rede P2P)
    3. Offline Intelligence (IA 100% local para empresas)
    
    Example:
        >>> engine = LocalEngine()
        >>> if engine.check_ollama_available():
        ...     models = engine.list_models()
        ...     request = LocalInferenceRequest(
        ...         prompt="Write a function to calculate fibonacci",
        ...         model="deepseek-coder:7b"
        ...     )
        ...     response = engine.generate(request)
        ...     print(response.text)
    """
    
    def __init__(self, ollama_host: str = "http://localhost:11434"):
        """
        Inicializa o Local Engine.
        
        Args:
            ollama_host: URL do servidor Ollama (default: localhost:11434)
        """
        self.ollama_host = ollama_host.rstrip('/')
        self.available_models: List[OllamaModel] = []
        self._cache: Dict[str, Any] = {}
        
        # Tentar detectar Ollama na inicializa√ß√£o
        try:
            self.check_ollama_available()
        except OllamaNotAvailableError:
            print("[LOCAL ENGINE] ‚ö†Ô∏è  Ollama n√£o detectado. Instale: https://ollama.ai")
    
    def check_ollama_available(self) -> bool:
        """
        Verifica se Ollama est√° rodando.
        
        Returns:
            True se Ollama est√° dispon√≠vel, False caso contr√°rio
            
        Raises:
            OllamaNotAvailableError: Se Ollama n√£o est√° dispon√≠vel
        """
        try:
            response = requests.get(f"{self.ollama_host}/api/tags", timeout=2)
            if response.status_code == 200:
                print("[LOCAL ENGINE] ‚úÖ Ollama detectado e rodando")
                return True
            else:
                raise OllamaNotAvailableError(
                    f"Ollama respondeu com status {response.status_code}"
                )
        except requests.exceptions.RequestException as e:
            raise OllamaNotAvailableError(
                f"Ollama n√£o est√° rodando. Instale em: https://ollama.ai\n"
                f"Erro: {e}"
            )
    
    def list_models(self) -> List[OllamaModel]:
        """
        Lista todos os modelos instalados no Ollama.
        
        Returns:
            Lista de modelos dispon√≠veis
            
        Example:
            >>> engine = LocalEngine()
            >>> models = engine.list_models()
            >>> for model in models:
            ...     print(f"{model.name} - {model.size_gb:.1f}GB")
        """
        try:
            response = requests.get(f"{self.ollama_host}/api/tags", timeout=5)
            response.raise_for_status()
            
            data = response.json()
            models = []
            
            for model_data in data.get('models', []):
                # Extrair informa√ß√µes do modelo
                name = model_data.get('name', 'unknown')
                size_bytes = model_data.get('size', 0)
                size_gb = size_bytes / (1024 ** 3)
                modified_at = model_data.get('modified_at')
                
                # Extrair fam√≠lia do modelo
                family = name.split(':')[0] if ':' in name else name
                
                # Estimar par√¢metros baseado no tamanho
                # Aproxima√ß√£o: 1B par√¢metros ‚âà 2GB (FP16)
                parameters = int((size_gb / 2) * 1_000_000_000)
                
                model = OllamaModel(
                    name=name,
                    size_gb=size_gb,
                    parameters=parameters,
                    installed=True,
                    family=family,
                    modified_at=modified_at
                )
                models.append(model)
            
            self.available_models = models
            
            print(f"[LOCAL ENGINE] üìö {len(models)} modelos instalados:")
            for model in models:
                print(f"  ‚Ä¢ {model.name} ({model.size_gb:.1f}GB, ~{model.parameters/1e9:.1f}B params)")
            
            return models
            
        except requests.exceptions.RequestException as e:
            raise OllamaNotAvailableError(f"Erro ao listar modelos: {e}")
    
    def generate(self, request: LocalInferenceRequest) -> LocalInferenceResponse:
        """
        Gera resposta usando modelo local.
        
        Args:
            request: Requisi√ß√£o de infer√™ncia
            
        Returns:
            Resposta do modelo com texto gerado e m√©tricas
            
        Raises:
            ModelNotFoundError: Se modelo n√£o est√° instalado
            OllamaNotAvailableError: Se Ollama n√£o est√° dispon√≠vel
            
        Example:
            >>> request = LocalInferenceRequest(
            ...     prompt="Explain quantum computing",
            ...     model="llama3:8b",
            ...     temperature=0.7
            ... )
            >>> response = engine.generate(request)
            >>> print(f"Generated {response.tokens_generated} tokens in {response.latency_ms:.0f}ms")
        """
        start_time = time.time()
        
        # Verificar se modelo est√° instalado
        if not self._is_model_installed(request.model):
            raise ModelNotFoundError(
                f"Modelo '{request.model}' n√£o est√° instalado. "
                f"Instale com: ollama pull {request.model}"
            )
        
        # Preparar payload
        payload = {
            "model": request.model,
            "prompt": request.prompt,
            "stream": False,  # N√£o usar streaming neste m√©todo
            "options": {
                "temperature": request.temperature,
                "num_predict": request.max_tokens
            }
        }
        
        if request.system:
            payload["system"] = request.system
        
        if request.context:
            payload["context"] = request.context
        
        try:
            # Fazer requisi√ß√£o para Ollama
            response = requests.post(
                f"{self.ollama_host}/api/generate",
                json=payload,
                timeout=120  # 2 minutos timeout
            )
            response.raise_for_status()
            
            data = response.json()
            
            # Calcular m√©tricas
            end_time = time.time()
            latency_ms = (end_time - start_time) * 1000
            
            # Extrair resposta
            text = data.get('response', '')
            context = data.get('context', [])
            done = data.get('done', True)
            
            # Estimar tokens gerados (aproxima√ß√£o: 1 token ‚âà 4 caracteres)
            tokens_generated = len(text) // 4
            tokens_per_second = tokens_generated / (latency_ms / 1000) if latency_ms > 0 else 0
            
            return LocalInferenceResponse(
                text=text,
                model=request.model,
                tokens_generated=tokens_generated,
                latency_ms=latency_ms,
                tokens_per_second=tokens_per_second,
                context=context,
                done=done
            )
            
        except requests.exceptions.Timeout:
            raise OllamaNotAvailableError(
                f"Timeout ao gerar resposta com modelo '{request.model}'. "
                f"Modelo pode ser muito grande ou prompt muito complexo."
            )
        except requests.exceptions.RequestException as e:
            raise OllamaNotAvailableError(f"Erro ao gerar resposta: {e}")
    
    def stream_generate(self, request: LocalInferenceRequest) -> Iterator[str]:
        """
        Gera resposta com streaming para UX responsiva.
        
        Args:
            request: Requisi√ß√£o de infer√™ncia (stream ser√° for√ßado para True)
            
        Yields:
            Tokens gerados incrementalmente
            
        Example:
            >>> request = LocalInferenceRequest(
            ...     prompt="Write a story",
            ...     model="llama3:8b"
            ... )
            >>> for token in engine.stream_generate(request):
            ...     print(token, end='', flush=True)
        """
        # Verificar se modelo est√° instalado
        if not self._is_model_installed(request.model):
            raise ModelNotFoundError(
                f"Modelo '{request.model}' n√£o est√° instalado. "
                f"Instale com: ollama pull {request.model}"
            )
        
        # Preparar payload
        payload = {
            "model": request.model,
            "prompt": request.prompt,
            "stream": True,  # For√ßar streaming
            "options": {
                "temperature": request.temperature,
                "num_predict": request.max_tokens
            }
        }
        
        if request.system:
            payload["system"] = request.system
        
        if request.context:
            payload["context"] = request.context
        
        try:
            # Fazer requisi√ß√£o com streaming
            response = requests.post(
                f"{self.ollama_host}/api/generate",
                json=payload,
                stream=True,
                timeout=120
            )
            response.raise_for_status()
            
            # Processar stream
            for line in response.iter_lines():
                if line:
                    data = json.loads(line)
                    token = data.get('response', '')
                    if token:
                        yield token
                    
                    # Verificar se terminou
                    if data.get('done', False):
                        break
                        
        except requests.exceptions.RequestException as e:
            raise OllamaNotAvailableError(f"Erro no streaming: {e}")
    
    def pull_model(self, model_name: str) -> None:
        """
        Baixa e instala novo modelo.
        
        Args:
            model_name: Nome do modelo (ex: "deepseek-coder:7b")
            
        Example:
            >>> engine.pull_model("deepseek-coder:7b")
            [LOCAL ENGINE] üì• Baixando deepseek-coder:7b...
            [LOCAL ENGINE] ‚úÖ Modelo instalado com sucesso
        """
        print(f"[LOCAL ENGINE] üì• Baixando {model_name}...")
        
        try:
            response = requests.post(
                f"{self.ollama_host}/api/pull",
                json={"name": model_name},
                stream=True,
                timeout=3600  # 1 hora para download
            )
            response.raise_for_status()
            
            # Processar progresso
            for line in response.iter_lines():
                if line:
                    data = json.loads(line)
                    status = data.get('status', '')
                    
                    if 'pulling' in status.lower():
                        # Mostrar progresso
                        total = data.get('total', 0)
                        completed = data.get('completed', 0)
                        if total > 0:
                            percent = (completed / total) * 100
                            print(f"[LOCAL ENGINE] üìä Progresso: {percent:.1f}%", end='\r')
                    
                    if data.get('status') == 'success':
                        print(f"\n[LOCAL ENGINE] ‚úÖ Modelo {model_name} instalado com sucesso")
                        break
                        
        except requests.exceptions.RequestException as e:
            raise OllamaNotAvailableError(f"Erro ao baixar modelo: {e}")
    
    def get_model_info(self, model_name: str) -> OllamaModel:
        """
        Retorna informa√ß√µes sobre modelo espec√≠fico.
        
        Args:
            model_name: Nome do modelo
            
        Returns:
            Informa√ß√µes do modelo
            
        Raises:
            ModelNotFoundError: Se modelo n√£o est√° instalado
        """
        # Atualizar lista de modelos
        self.list_models()
        
        # Buscar modelo
        for model in self.available_models:
            if model.name == model_name:
                return model
        
        raise ModelNotFoundError(f"Modelo '{model_name}' n√£o encontrado")
    
    def _is_model_installed(self, model_name: str) -> bool:
        """
        Verifica se modelo est√° instalado.
        
        Args:
            model_name: Nome do modelo
            
        Returns:
            True se instalado, False caso contr√°rio
        """
        # Atualizar lista se vazia
        if not self.available_models:
            try:
                self.list_models()
            except:
                return False
        
        # Verificar se modelo est√° na lista
        return any(m.name == model_name for m in self.available_models)
    
    def get_recommended_models(self) -> Dict[str, str]:
        """
        Retorna modelos recomendados para diferentes casos de uso.
        
        Returns:
            Dicion√°rio com recomenda√ß√µes
        """
        return {
            "coding": "deepseek-coder:7b",
            "general": "llama3:8b",
            "fast": "mistral:7b",
            "powerful": "llama3:70b",
            "code_large": "deepseek-coder:33b"
        }
    
    def get_statistics(self) -> Dict[str, Any]:
        """
        Retorna estat√≠sticas do Local Engine.
        
        Returns:
            Estat√≠sticas de uso
        """
        return {
            "ollama_available": self.check_ollama_available(),
            "models_installed": len(self.available_models),
            "total_size_gb": sum(m.size_gb for m in self.available_models),
            "models": [m.to_dict() for m in self.available_models]
        }


# Singleton instance
_local_engine: Optional[LocalEngine] = None


def get_local_engine() -> LocalEngine:
    """
    Retorna inst√¢ncia singleton do Local Engine.
    
    Returns:
        LocalEngine singleton
    """
    global _local_engine
    if _local_engine is None:
        _local_engine = LocalEngine()
    return _local_engine


if __name__ == "__main__":
    # Demo r√°pido
    print("=" * 80)
    print("AETHEL LOCAL ENGINE - DEMO")
    print("=" * 80)
    
    engine = LocalEngine()
    
    try:
        # Verificar Ollama
        engine.check_ollama_available()
        
        # Listar modelos
        models = engine.list_models()
        
        if models:
            # Testar infer√™ncia com primeiro modelo
            model = models[0]
            print(f"\n[DEMO] Testando infer√™ncia com {model.name}...")
            
            request = LocalInferenceRequest(
                prompt="Write a Python function to calculate factorial",
                model=model.name,
                temperature=0.7,
                max_tokens=200
            )
            
            response = engine.generate(request)
            
            print(f"\n[DEMO] Resposta gerada:")
            print(response.text)
            print(f"\n[DEMO] M√©tricas:")
            print(f"  ‚Ä¢ Tokens: {response.tokens_generated}")
            print(f"  ‚Ä¢ Lat√™ncia: {response.latency_ms:.0f}ms")
            print(f"  ‚Ä¢ Throughput: {response.tokens_per_second:.1f} tokens/s")
        else:
            print("\n[DEMO] Nenhum modelo instalado.")
            print("[DEMO] Instale um modelo com: ollama pull deepseek-coder:7b")
            
    except OllamaNotAvailableError as e:
        print(f"\n[DEMO] ‚ùå {e}")
        print("\n[DEMO] Para instalar Ollama:")
        print("  1. Visite: https://ollama.ai")
        print("  2. Baixe e instale para seu sistema operacional")
        print("  3. Execute: ollama pull deepseek-coder:7b")
        print("  4. Execute este script novamente")
