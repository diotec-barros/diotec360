"""
Copyright 2024 Dion√≠sio Sebasti√£o Barros / DIOTEC 360

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
"""

"""
Aethel LoRA Trainer - Fine-Tuning Aut√¥nomo
Sistema que treina o modelo local com respostas verificadas.

Este m√≥dulo implementa o ciclo de aprendizado do Neural Nexus: ele pega respostas
verificadas da Cognitive Persistence e usa LoRA (Low-Rank Adaptation) para treinar
o modelo local, tornando-o t√£o inteligente quanto os "professores" (GPT-4, Claude).

Research Foundation:
- LoRA: Low-Rank Adaptation of Large Language Models (Hu et al., 2021)
- Knowledge Distillation (Hinton et al., 2015)
- Unsloth: Fast LoRA training library

Author: Kiro AI - Engenheiro-Chefe
Version: Epoch 4.0 "Neural Nexus"
Date: February 18, 2026
"""

import json
import time
import os
from dataclasses import dataclass, asdict
from typing import List, Optional, Dict, Any, Tuple
from pathlib import Path
from datetime import datetime


@dataclass
class LoRAConfig:
    """
    Configura√ß√£o de treinamento LoRA.
    
    Attributes:
        model_name: Nome do modelo base (ex: "deepseek-coder:7b")
        dataset_path: Caminho do dataset (JSON Lines)
        output_dir: Diret√≥rio de sa√≠da para modelo treinado
        lora_rank: Rank da matriz LoRA (default: 8)
        lora_alpha: Alpha scaling factor (default: 16)
        learning_rate: Taxa de aprendizado (default: 3e-4)
        batch_size: Tamanho do batch (default: 4)
        num_epochs: N√∫mero de √©pocas (default: 3)
        max_seq_length: Comprimento m√°ximo de sequ√™ncia (default: 2048)
        warmup_steps: Steps de warmup (default: 100)
        save_steps: Salvar checkpoint a cada N steps (default: 500)
        eval_steps: Avaliar a cada N steps (default: 100)
    """
    model_name: str
    dataset_path: str
    output_dir: str = "./lora_models"
    lora_rank: int = 8
    lora_alpha: int = 16
    learning_rate: float = 3e-4
    batch_size: int = 4
    num_epochs: int = 3
    max_seq_length: int = 2048
    warmup_steps: int = 100
    save_steps: int = 500
    eval_steps: int = 100


@dataclass
class TrainingMetrics:
    """
    M√©tricas de treinamento.
    
    Attributes:
        epoch: √âpoca atual
        step: Step atual
        loss: Loss atual
        learning_rate: Learning rate atual
        tokens_per_second: Throughput
        timestamp: Timestamp
    """
    epoch: int
    step: int
    loss: float
    learning_rate: float
    tokens_per_second: float
    timestamp: float


@dataclass
class ModelVersion:
    """
    Vers√£o de modelo treinado.
    
    Attributes:
        version: N√∫mero da vers√£o
        base_model: Modelo base usado
        training_date: Data do treinamento
        num_examples: N√∫mero de exemplos usados
        num_epochs: N√∫mero de √©pocas
        final_loss: Loss final
        validation_accuracy: Accuracy no conjunto de valida√ß√£o
        model_path: Caminho do modelo
        config: Configura√ß√£o usada
    """
    version: int
    base_model: str
    training_date: float
    num_examples: int
    num_epochs: int
    final_loss: float
    validation_accuracy: float
    model_path: str
    config: Dict[str, Any]


class LoRATrainer:
    """
    LoRA Trainer - Fine-Tuning Aut√¥nomo do Modelo Local.
    
    Este sistema implementa o ciclo de aprendizado do Neural Nexus:
    1. Carrega dataset de respostas verificadas
    2. Prepara dados para treinamento
    3. Treina modelo usando LoRA
    4. Valida performance
    5. Deploy se accuracy melhorou
    
    LoRA (Low-Rank Adaptation) permite treinar modelos grandes com:
    - Baixo uso de mem√≥ria (apenas 1-2% dos par√¢metros)
    - Treinamento r√°pido (10x mais r√°pido que full fine-tuning)
    - Qualidade compar√°vel a full fine-tuning
    
    Example:
        >>> trainer = LoRATrainer(local_engine, persistence)
        >>> if trainer.should_train():
        ...     config = LoRAConfig(
        ...         model_name="deepseek-coder:7b",
        ...         dataset_path="./lora_dataset.jsonl"
        ...     )
        ...     version = trainer.train(config)
        ...     if version.validation_accuracy > 0.9:
        ...         trainer.deploy(version)
    """
    
    def __init__(self, local_engine, cognitive_persistence):
        """
        Inicializa LoRA Trainer.
        
        Args:
            local_engine: LocalEngine instance
            cognitive_persistence: CognitivePersistence instance
        """
        self.local_engine = local_engine
        self.persistence = cognitive_persistence
        
        # Hist√≥rico de treinamento
        self.training_history: List[TrainingMetrics] = []
        self.model_versions: List[ModelVersion] = []
        
        # Carregar vers√µes existentes
        self._load_versions()
        
        print("[LORA] üéì LoRA Trainer inicializado")
        print(f"  Vers√µes existentes: {len(self.model_versions)}")
    
    def should_train(self, min_examples: int = 1000, min_confidence: float = 0.8) -> bool:
        """
        Verifica se h√° exemplos suficientes para treinar.
        
        Args:
            min_examples: M√≠nimo de exemplos necess√°rios
            min_confidence: Confian√ßa m√≠nima dos exemplos
        
        Returns:
            True se pronto para treinar
        """
        readiness = self.persistence.get_training_readiness()
        
        if readiness['ready']:
            print(f"[LORA] ‚úÖ Dataset pronto para treinamento!")
            print(f"  Exemplos de alta qualidade: {readiness['high_quality_count']}")
            return True
        else:
            print(f"[LORA] ‚è≥ Dataset n√£o pronto")
            print(f"  {readiness['message']}")
            return False
    
    def prepare_dataset(self, output_path: str, min_confidence: float = 0.8,
                       train_split: float = 0.9) -> Tuple[str, str]:
        """
        Prepara dataset para treinamento (train/val split).
        
        Args:
            output_path: Caminho base para datasets
            min_confidence: Confian√ßa m√≠nima
            train_split: Propor√ß√£o de treino (0.9 = 90% treino, 10% valida√ß√£o)
        
        Returns:
            Tupla (train_path, val_path)
        """
        print(f"\n[LORA] üìä Preparando dataset...")
        
        # Exportar dataset completo
        full_path = output_path + ".full.jsonl"
        count = self.persistence.export_for_lora(full_path, min_confidence)
        
        if count == 0:
            raise Exception("Nenhum exemplo para treinar")
        
        # Calcular split
        train_count = int(count * train_split)
        val_count = count - train_count
        
        print(f"[LORA] üìà Split: {train_count} treino, {val_count} valida√ß√£o")
        
        # Criar arquivos de treino e valida√ß√£o
        train_path = output_path + ".train.jsonl"
        val_path = output_path + ".val.jsonl"
        
        with open(full_path, 'r', encoding='utf-8') as f_in:
            lines = f_in.readlines()
            
            with open(train_path, 'w', encoding='utf-8') as f_train:
                f_train.writelines(lines[:train_count])
            
            with open(val_path, 'w', encoding='utf-8') as f_val:
                f_val.writelines(lines[train_count:])
        
        print(f"[LORA] ‚úÖ Dataset preparado")
        print(f"  Train: {train_path}")
        print(f"  Val: {val_path}")
        
        return train_path, val_path
    
    def train(self, config: LoRAConfig) -> ModelVersion:
        """
        Executa treinamento LoRA.
        
        Args:
            config: Configura√ß√£o de treinamento
        
        Returns:
            Vers√£o do modelo treinado
        
        Note:
            Esta √© uma implementa√ß√£o mock. Para produ√ß√£o, integrar com:
            - Unsloth (https://github.com/unslothai/unsloth)
            - Hugging Face PEFT
            - Ollama fine-tuning API
        """
        print(f"\n[LORA] üöÄ Iniciando treinamento...")
        print(f"  Modelo base: {config.model_name}")
        print(f"  Dataset: {config.dataset_path}")
        print(f"  LoRA rank: {config.lora_rank}")
        print(f"  Learning rate: {config.learning_rate}")
        print(f"  Epochs: {config.num_epochs}")
        
        start_time = time.time()
        
        # Verificar se modelo base existe
        try:
            model_info = self.local_engine.get_model_info(config.model_name)
            print(f"[LORA] ‚úÖ Modelo base encontrado ({model_info.size_gb:.1f}GB)")
        except Exception as e:
            raise Exception(f"Modelo base n√£o encontrado: {e}")
        
        # Carregar dataset
        num_examples = self._count_examples(config.dataset_path)
        print(f"[LORA] üìö {num_examples} exemplos carregados")
        
        # Criar diret√≥rio de sa√≠da
        output_dir = Path(config.output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # Gerar vers√£o
        version_num = len(self.model_versions) + 1
        version_dir = output_dir / f"v{version_num}"
        version_dir.mkdir(parents=True, exist_ok=True)
        
        # Simular treinamento (mock)
        print(f"\n[LORA] üîÑ Treinando...")
        
        total_steps = (num_examples // config.batch_size) * config.num_epochs
        current_step = 0
        
        for epoch in range(config.num_epochs):
            print(f"\n[LORA] üìñ Epoch {epoch + 1}/{config.num_epochs}")
            
            # Simular steps
            steps_per_epoch = num_examples // config.batch_size
            
            for step in range(steps_per_epoch):
                current_step += 1
                
                # Simular loss decrescente
                loss = 2.0 * (1.0 - (current_step / total_steps)) + 0.1
                
                # Simular learning rate com warmup
                if current_step < config.warmup_steps:
                    lr = config.learning_rate * (current_step / config.warmup_steps)
                else:
                    lr = config.learning_rate
                
                # Simular throughput
                tokens_per_second = 1000 + (current_step % 500)
                
                # Registrar m√©tricas
                metrics = TrainingMetrics(
                    epoch=epoch + 1,
                    step=current_step,
                    loss=loss,
                    learning_rate=lr,
                    tokens_per_second=tokens_per_second,
                    timestamp=time.time()
                )
                self.training_history.append(metrics)
                
                # Log a cada 10% do epoch
                if step % (steps_per_epoch // 10) == 0:
                    progress = (step / steps_per_epoch) * 100
                    print(f"  Step {step}/{steps_per_epoch} ({progress:.0f}%) - "
                          f"Loss: {loss:.4f} - LR: {lr:.6f}")
        
        # Loss final
        final_loss = self.training_history[-1].loss if self.training_history else 0.5
        
        # Simular valida√ß√£o
        print(f"\n[LORA] üß™ Validando modelo...")
        validation_accuracy = self._validate_model(config)
        
        print(f"[LORA] üìä Validation accuracy: {validation_accuracy:.1%}")
        
        # Salvar modelo (mock - apenas criar arquivo de metadados)
        model_path = str(version_dir / "adapter_model.bin")
        self._save_model_metadata(model_path, config)
        
        # Criar vers√£o
        version = ModelVersion(
            version=version_num,
            base_model=config.model_name,
            training_date=time.time(),
            num_examples=num_examples,
            num_epochs=config.num_epochs,
            final_loss=final_loss,
            validation_accuracy=validation_accuracy,
            model_path=str(version_dir),
            config=asdict(config)
        )
        
        # Salvar vers√£o
        self.model_versions.append(version)
        self._save_versions()
        
        elapsed_time = time.time() - start_time
        
        print(f"\n[LORA] ‚úÖ Treinamento completo em {elapsed_time:.0f}s")
        print(f"  Vers√£o: v{version_num}")
        print(f"  Loss final: {final_loss:.4f}")
        print(f"  Validation accuracy: {validation_accuracy:.1%}")
        print(f"  Modelo salvo em: {version_dir}")
        
        return version
    
    def _count_examples(self, dataset_path: str) -> int:
        """Conta n√∫mero de exemplos no dataset"""
        count = 0
        with open(dataset_path, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    count += 1
        return count
    
    def _validate_model(self, config: LoRAConfig) -> float:
        """
        Valida modelo no conjunto de valida√ß√£o.
        
        Returns:
            Accuracy (0.0-1.0)
        """
        # Mock: simular accuracy baseada em configura√ß√£o
        # Em produ√ß√£o, executar infer√™ncia no conjunto de valida√ß√£o
        
        # Accuracy aumenta com mais epochs e exemplos
        base_accuracy = 0.75
        epoch_bonus = config.num_epochs * 0.03
        rank_bonus = (config.lora_rank / 16) * 0.05
        
        accuracy = min(0.95, base_accuracy + epoch_bonus + rank_bonus)
        
        return accuracy
    
    def _save_model_metadata(self, model_path: str, config: LoRAConfig) -> None:
        """Salva metadados do modelo"""
        metadata = {
            "base_model": config.model_name,
            "lora_rank": config.lora_rank,
            "lora_alpha": config.lora_alpha,
            "training_date": datetime.now().isoformat(),
            "config": asdict(config)
        }
        
        metadata_path = Path(model_path).parent / "metadata.json"
        with open(metadata_path, 'w') as f:
            json.dump(metadata, f, indent=2)
    
    def deploy(self, version: ModelVersion, force: bool = False) -> bool:
        """
        Deploy modelo treinado para produ√ß√£o.
        
        Args:
            version: Vers√£o do modelo
            force: For√ßar deploy mesmo se accuracy n√£o melhorou
        
        Returns:
            True se deploy foi bem-sucedido
        """
        print(f"\n[LORA] üöÄ Deploying v{version.version}...")
        
        # Verificar se accuracy melhorou
        if not force and self.model_versions:
            current_best = max(self.model_versions[:-1],
                             key=lambda v: v.validation_accuracy,
                             default=None)
            
            if current_best and version.validation_accuracy <= current_best.validation_accuracy:
                print(f"[LORA] ‚ö†Ô∏è  Accuracy n√£o melhorou")
                print(f"  Atual: {current_best.validation_accuracy:.1%}")
                print(f"  Nova: {version.validation_accuracy:.1%}")
                print(f"  Use force=True para deploy mesmo assim")
                return False
        
        # Em produ√ß√£o, aqui faria:
        # 1. Carregar adapter LoRA
        # 2. Mesclar com modelo base
        # 3. Exportar para formato Ollama
        # 4. Substituir modelo no Ollama
        
        print(f"[LORA] ‚úÖ Deploy completo")
        print(f"  Modelo: {version.base_model}")
        print(f"  Vers√£o: v{version.version}")
        print(f"  Accuracy: {version.validation_accuracy:.1%}")
        
        return True
    
    def rollback(self, version_num: int) -> bool:
        """
        Reverte para vers√£o anterior.
        
        Args:
            version_num: N√∫mero da vers√£o
        
        Returns:
            True se rollback foi bem-sucedido
        """
        # Buscar vers√£o
        version = next((v for v in self.model_versions if v.version == version_num), None)
        
        if not version:
            print(f"[LORA] ‚ùå Vers√£o v{version_num} n√£o encontrada")
            return False
        
        print(f"[LORA] ‚è™ Rollback para v{version_num}...")
        
        # Em produ√ß√£o, restaurar modelo da vers√£o
        
        print(f"[LORA] ‚úÖ Rollback completo")
        return True
    
    def get_best_version(self) -> Optional[ModelVersion]:
        """
        Retorna vers√£o com maior accuracy.
        
        Returns:
            Melhor vers√£o ou None
        """
        if not self.model_versions:
            return None
        
        return max(self.model_versions, key=lambda v: v.validation_accuracy)
    
    def get_statistics(self) -> Dict[str, Any]:
        """
        Retorna estat√≠sticas de treinamento.
        
        Returns:
            Estat√≠sticas completas
        """
        if not self.model_versions:
            return {
                "total_versions": 0,
                "best_accuracy": 0.0,
                "total_training_time": 0.0
            }
        
        best = self.get_best_version()
        
        return {
            "total_versions": len(self.model_versions),
            "best_version": best.version if best else 0,
            "best_accuracy": best.validation_accuracy if best else 0.0,
            "total_examples_trained": sum(v.num_examples for v in self.model_versions),
            "average_accuracy": sum(v.validation_accuracy for v in self.model_versions) / len(self.model_versions),
            "versions": [
                {
                    "version": v.version,
                    "accuracy": v.validation_accuracy,
                    "loss": v.final_loss,
                    "examples": v.num_examples,
                    "date": datetime.fromtimestamp(v.training_date).isoformat()
                }
                for v in self.model_versions
            ]
        }
    
    def _load_versions(self) -> None:
        """Carrega vers√µes existentes do disco"""
        versions_file = Path(".aethel_lora/versions.json")
        
        if versions_file.exists():
            with open(versions_file, 'r') as f:
                data = json.load(f)
                self.model_versions = [
                    ModelVersion(**v) for v in data.get('versions', [])
                ]
    
    def _save_versions(self) -> None:
        """Salva vers√µes no disco"""
        versions_file = Path(".aethel_lora/versions.json")
        versions_file.parent.mkdir(parents=True, exist_ok=True)
        
        data = {
            "versions": [asdict(v) for v in self.model_versions],
            "last_updated": time.time()
        }
        
        with open(versions_file, 'w') as f:
            json.dump(data, f, indent=2)


def create_trainer_from_env():
    """
    Cria LoRA Trainer a partir de vari√°veis de ambiente.
    
    Returns:
        LoRATrainer instance
    """
    from diotec360.ai.local_engine import LocalEngine
    from diotec360.ai.cognitive_persistence import CognitivePersistence
    
    local_engine = LocalEngine()
    persistence = CognitivePersistence()
    
    return LoRATrainer(local_engine, persistence)


if __name__ == "__main__":
    # Demo r√°pido
    print("=" * 80)
    print("AETHEL LORA TRAINER - DEMO")
    print("=" * 80)
    
    # Criar trainer
    trainer = create_trainer_from_env()
    
    # Verificar se pronto para treinar
    if trainer.should_train():
        print("\n[DEMO] ‚úÖ Dataset pronto para treinamento!")
    else:
        print("\n[DEMO] ‚è≥ Dataset n√£o pronto ainda")
    
    # Estat√≠sticas
    stats = trainer.get_statistics()
    print(f"\n[DEMO] Estat√≠sticas:")
    print(f"  Total de vers√µes: {stats['total_versions']}")
    print(f"  Melhor accuracy: {stats['best_accuracy']:.1%}")
    
    print("\nüèõÔ∏è [LORA TRAINER: OPERATIONAL]")
