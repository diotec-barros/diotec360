"""
Copyright 2024 Dion√≠sio Sebasti√£o Barros / DIOTEC 360

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
"""

import os
from datetime import datetime
from diotec360.core.parser import AethelParser
from diotec360.core.bridge import AethelBridge
from diotec360.core.judge import AethelJudge


class AethelGenerator:
    """
    Orquestrador principal que conecta Parser -> Bridge -> AI -> Artifact
    """
    
    def __init__(self, ai_provider="anthropic", enable_verification=True):
        self.parser = AethelParser()
        self.ai_provider = ai_provider
        self.enable_verification = enable_verification
        self._validate_api_keys()
    
    def _validate_api_keys(self):
        """Verifica se as chaves de API est√£o configuradas"""
        if self.ai_provider == "anthropic":
            self.api_key = os.getenv("ANTHROPIC_API_KEY")
            if not self.api_key:
                print("‚ö†Ô∏è  ANTHROPIC_API_KEY n√£o encontrada. Configure para usar gera√ß√£o real.")
        elif self.ai_provider == "openai":
            self.api_key = os.getenv("OPENAI_API_KEY")
            if not self.api_key:
                print("‚ö†Ô∏è  OPENAI_API_KEY n√£o encontrada. Configure para usar gera√ß√£o real.")
        elif self.ai_provider == "ollama":
            self.api_key = None  # Ollama n√£o precisa de chave
            print("ü¶ô Usando Ollama local")
    
    def compile(self, aethel_code, intent_name=None, output_file=None):
        """
        Pipeline completo: Aethel -> AST -> Verification -> Prompt -> AI -> Rust
        """
        print("üîç [1/6] Parsing c√≥digo Aethel...")
        ast = self.parser.parse(aethel_code)
        
        # Se n√£o especificou intent, usa o primeiro
        if intent_name is None:
            intent_name = list(ast.keys())[0]
        
        # VERIFICA√á√ÉO FORMAL ANTES DA GERA√á√ÉO
        verification_result = None
        if self.enable_verification:
            print(f"‚öñÔ∏è  [2/6] Verifica√ß√£o formal da l√≥gica...")
            judge = AethelJudge(ast)
            verification_result = judge.verify_logic(intent_name)
            
            if verification_result['status'] == 'FAILED':
                print("\n‚ùå COMPILA√á√ÉO ABORTADA!")
                print("O Juiz detectou falhas l√≥gicas nas constraints.")
                report = judge.generate_proof_report(intent_name, verification_result)
                print(report)
                
                return {
                    "status": "FAILED",
                    "ast": ast,
                    "verification": verification_result,
                    "report": report
                }
            else:
                print(f"‚úÖ Verifica√ß√£o formal: {verification_result['message']}")
        
        print(f"üåâ [3/6] Construindo ponte para intent '{intent_name}'...")
        bridge = AethelBridge(ast)
        prompt = bridge.generate_generation_prompt(intent_name)
        
        print("ü§ñ [4/6] Enviando para gerador de IA...")
        generated_code = self._call_ai(prompt)
        
        print("üì¶ [5/6] Empacotando artefato final...")
        artifact = bridge.build_final_artifact(generated_code, intent_name)
        artifact = artifact.replace("{timestamp}", datetime.now().isoformat())
        
        print("üíæ [6/6] Salvando c√≥digo gerado...")
        if output_file:
            with open(output_file, 'w') as f:
                f.write(artifact)
            print(f"‚úÖ C√≥digo salvo em: {output_file}")
        
        # Gerar relat√≥rio de verifica√ß√£o se habilitado
        report = None
        if self.enable_verification and verification_result:
            report = judge.generate_proof_report(intent_name, verification_result)
        
        return {
            "status": "SUCCESS",
            "ast": ast,
            "verification": verification_result,
            "prompt": prompt,
            "generated_code": generated_code,
            "artifact": artifact,
            "report": report
        }
    
    def _call_ai(self, prompt):
        """
        Chama a API de IA escolhida
        """
        if not self.api_key and self.ai_provider != "ollama":
            print("‚ö†Ô∏è  Modo simula√ß√£o (sem API key)")
            return self._simulate_generation()
        
        if self.ai_provider == "anthropic":
            return self._call_anthropic(prompt)
        elif self.ai_provider == "openai":
            return self._call_openai(prompt)
        elif self.ai_provider == "ollama":
            return self._call_ollama(prompt)
    
    def _call_anthropic(self, prompt):
        """Chama Claude via Anthropic API"""
        try:
            import anthropic
            client = anthropic.Anthropic(api_key=self.api_key)
            
            message = client.messages.create(
                model="claude-3-5-sonnet-20241022",
                max_tokens=2048,
                messages=[
                    {"role": "user", "content": prompt}
                ]
            )
            
            return message.content[0].text
        except ImportError:
            print("‚ö†Ô∏è  Biblioteca 'anthropic' n√£o instalada. Execute: pip install anthropic")
            return self._simulate_generation()
        except Exception as e:
            print(f"‚ùå Erro ao chamar Anthropic: {e}")
            return self._simulate_generation()
    
    def _call_openai(self, prompt):
        """Chama GPT via OpenAI API"""
        try:
            import openai
            client = openai.OpenAI(api_key=self.api_key)
            
            response = client.chat.completions.create(
                model="gpt-4",
                messages=[
                    {"role": "system", "content": "You are an expert Rust code generator."},
                    {"role": "user", "content": prompt}
                ]
            )
            
            return response.choices[0].message.content
        except ImportError:
            print("‚ö†Ô∏è  Biblioteca 'openai' n√£o instalada. Execute: pip install openai")
            return self._simulate_generation()
        except Exception as e:
            print(f"‚ùå Erro ao chamar OpenAI: {e}")
            return self._simulate_generation()
    
    def _call_ollama(self, prompt):
        """Chama modelo local via Ollama"""
        try:
            import requests
            
            response = requests.post(
                "http://localhost:11434/api/generate",
                json={
                    "model": "codellama",
                    "prompt": prompt,
                    "stream": False
                }
            )
            
            return response.json()["response"]
        except ImportError:
            print("‚ö†Ô∏è  Biblioteca 'requests' n√£o instalada. Execute: pip install requests")
            return self._simulate_generation()
        except Exception as e:
            print(f"‚ùå Erro ao chamar Ollama: {e}")
            return self._simulate_generation()
    
    def _simulate_generation(self):
        """Simula√ß√£o para quando n√£o h√° API dispon√≠vel"""
        return """fn transfer_funds(sender: &mut Account, receiver: &mut Account, amount: Gold) -> Result<(), TransferError> {
    // Guard: Valida√ß√£o de pr√©-condi√ß√µes
    if sender.balance < amount {
        return Err(TransferError::InsufficientFunds);
    }
    if amount <= 0 {
        return Err(TransferError::InvalidAmount);
    }
    
    let old_balance = sender.balance;
    
    // Solve: Execu√ß√£o otimizada para blockchain
    sender.balance -= amount;
    receiver.balance += amount;
    
    // Verify: Valida√ß√£o de p√≥s-condi√ß√µes
    assert!(sender.balance < old_balance);
    
    Ok(())
}"""
